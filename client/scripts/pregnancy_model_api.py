# # -*- coding: utf-8 -*-
# """pregnancy_risk_detection

# Automatically generated by Colab.

# Original file is located at
#     https://colab.research.google.com/drive/1O50o0IoAnHUvYv7LPXnyeVw5o7kbUIPR
# """

# import pandas as pd
# import numpy as np
# import seaborn as sns
# import matplotlib.pyplot as plt
# from scipy.stats import zscore

# df = pd.read_csv("/content/Maternal_Health_Risk_Data_Set.csv")

# # Check for outliers using Z-score
# z_scores = np.abs(zscore(df.select_dtypes(include=[np.number])))  # Excluding non-numeric columns
# outliers = (z_scores > 3).sum(axis=0)  # Count outliers (Z-score > 3)
# print("Outlier Count Per Feature:\n", outliers)

# # Visualize outliers using box plots
# plt.figure(figsize=(12, 6))
# df.boxplot(rot=45)
# plt.title("Boxplot of Numerical Features (Checking for Outliers)")
# plt.show()

# import pandas as pd
# import numpy as np
# import seaborn as sns
# import matplotlib.pyplot as plt
# from sklearn.model_selection import train_test_split
# from sklearn.preprocessing import StandardScaler, LabelEncoder
# from sklearn.ensemble import RandomForestClassifier
# from sklearn.linear_model import LogisticRegression
# from sklearn.svm import SVC
# from xgboost import XGBClassifier
# from sklearn.metrics import accuracy_score, confusion_matrix

# # Encode categorical target variable (RiskLevel)
# label_encoder = LabelEncoder()
# df["RiskLevel"] = label_encoder.fit_transform(df["RiskLevel"])  # High/Mid/Low â†’ 2/1/0
# # Separate features & target
# X = df.drop(columns=["RiskLevel"])
# y = df["RiskLevel"]

# # Normalize features using StandardScaler
# scaler = StandardScaler()
# X_scaled = scaler.fit_transform(X)

# # Split dataset into 80% training and 20% testing
# X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.05, random_state=42)

# from sklearn.model_selection import GridSearchCV
# from xgboost import XGBClassifier

# # Update XGBoost model with best gamma
# xgb_final = XGBClassifier(
#     learning_rate=0.1,
#     n_estimators=2000,
#     max_depth=6,
#     min_child_weight=2,
#     #gamma=grid_search3.best_params_['gamma'],
#     gamma=0,
#     subsample=0.8,
#     colsample_bytree=0.8,
#     #objective='binary:logistic',
#     objective='multi:softmax',
#     num_class=3,
#     nthread=4,
#     reg_alpha=0.00005,
#     #scale_pos_weight=1,
#     seed=27
# )

# # Fit the final model
# xgb_final.fit(X_train, y_train)

# # Evaluate the model
# train_predictions = xgb_final.predict(X_train)
# test_predictions = xgb_final.predict(X_test)

# # Print accuracy scores
# from sklearn.metrics import accuracy_score
# print(f"Training Accuracy: {accuracy_score(y_train, train_predictions):.6f}")
# print(f"Testing Accuracy: {accuracy_score(y_test, test_predictions):.6f}")

# # Load new input data from CSV (provided by website)
# input_csv = "/content/input_data.csv"  # Replace with actual input file path
# output_csv = "/content/predictions.csv"  # Output file to store predictions

# try:
#     input_data = pd.read_csv(input_csv)

#     # Ensure input data has the same columns as training data
#     missing_cols = set(X.columns) - set(input_data.columns)
#     if missing_cols:
#         raise ValueError(f"Missing columns in input CSV: {missing_cols}")

#     # Scale input data using the same scaler
#     input_scaled = scaler.transform(input_data)

#     # Predict using trained model
#     predictions = xgb_final.predict(input_scaled)

#     # # Convert predictions back to original labels
#     # predicted_labels = label_encoder.inverse_transform(predictions)

#     # # Save predictions to a new CSV file
#     # output_df = input_data.copy()
#     # output_df["Predicted RiskLevel"] = predicted_labels
#     # output_df.to_csv(output_csv, index=False)

#     # print(f"Predictions saved successfully to {output_csv}")
#     # Convert predictions back to original labels
#     label_mapping = {0: "Low Risk", 1: "Mid Risk", 2: "High Risk"}
#     predicted_labels = [label_mapping[pred] for pred in predictions]

#     # Save predictions to a new CSV file
#     output_df = input_data.copy()
#     output_df["Predicted RiskLevel"] = predicted_labels
#     output_df.to_csv(output_csv, index=False)

#     print(f"Predictions saved successfully to {output_csv}")

# except Exception as e:
#     print(f"Error processing input CSV: {e}")

# pregnancy_model_api.py

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score
import joblib

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from fastapi.middleware.cors import CORSMiddleware

# ==================== Model Training ==================== #

# Load dataset
df = pd.read_csv("Maternal_Health_Risk_Data_Set.csv")

# Encode target
label_encoder = LabelEncoder()
df["RiskLevel"] = label_encoder.fit_transform(df["RiskLevel"])  # Low:0, Mid:1, High:2

# Split features and labels
X = df.drop(columns=["RiskLevel"])
y = df["RiskLevel"]

# Scale features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split into train/test
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.05, random_state=42)

# Train XGBoost model
xgb_model = XGBClassifier(
    learning_rate=0.1,
    n_estimators=2000,
    max_depth=6,
    min_child_weight=2,
    gamma=0,
    subsample=0.8,
    colsample_bytree=0.8,
    objective='multi:softmax',
    num_class=3,
    nthread=4,
    reg_alpha=0.00005,
    seed=27
)

xgb_model.fit(X_train, y_train)

# Evaluate
print("Training Accuracy:", accuracy_score(y_train, xgb_model.predict(X_train)))
print("Testing Accuracy:", accuracy_score(y_test, xgb_model.predict(X_test)))

# Save model and scaler
joblib.dump(xgb_model, "xgb_model.pkl")
joblib.dump(scaler, "scaler.pkl")

# ==================== FastAPI Setup ==================== #

# Load saved model and scaler
model = joblib.load("xgb_model.pkl")
scaler = joblib.load("scaler.pkl")

# Create FastAPI app
app = FastAPI()

# Enable CORS for React frontend
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Replace with your frontend URL in production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Input schema
class InputData(BaseModel):
    Age: float
    SystolicBP: float
    DiastolicBP: float
    BS: float  # Blood Sugar
    BodyTemp: float
    HeartRate: float
    Week: int  # Not used, but accepted

# Prediction endpoint
@app.post("/predict")
def predict_risk(data: InputData):
    try:
        # Prepare input
        input_df = pd.DataFrame([{
            "Age": data.Age,
            "SystolicBP": data.SystolicBP,
            "DiastolicBP": data.DiastolicBP,
            "BS": data.BS,
            "BodyTemp": data.BodyTemp,
            "HeartRate": data.HeartRate
        }])

        # Scale
        input_scaled = scaler.transform(input_df)

        # Predict
        prediction = model.predict(input_scaled)[0]
        label_map = {0: "Low Risk", 1: "Mid Risk", 2: "High Risk"}
        risk = label_map[prediction]

        return {"risk": risk}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
